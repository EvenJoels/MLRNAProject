{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ML Model To Predict NSUN6 Affected Genes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Imports & Set-Up\n",
    "\n",
    "Currently just using the 50:50 area selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyfaidx import Fasta\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import logomaker as lm\n",
    "import re\n",
    "\n",
    "dataset = pd.read_csv(\"out\\\\NSUN6_dataset_2405241153.csv\", index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Dealing With The \"p>>n\" Problem\n",
    "\n",
    "The current datasets created have 4756 features (n) but only 227 samples in the positive dataset and 75 samples in the negative dataset (p). To mitigate this imbalance which could lead to over-fitting and similar issues various methods should be considered to reduce the size of n (or increase the size of p).\n",
    "\n",
    "There is no best method and it is recommended to use controlled experiments to test a suite of different methods..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whilst the features 'A', 'C', 'T', 'G' are obviously crucial, we need to identify how to reduce the base pairing probability down to just the problem-significant components.\n",
    "\n",
    "From a quick inspection all base-pairing probabilities (bpp) have the same likelihood of being blank as each other, normally ~200/227 samples are blank!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dimensional Reduction [PCA, tSNE]\n",
    "\n",
    "PCA is less effective at preserving local structures. tSNE is frequently used for bioinformatics/biomedical signal processing but it requires hyperparameter like perplexity and number of steps.\n",
    "\n",
    "Only use the highly variable features???\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bpp_set = dataset.iloc[:,6:] # Only want to reduce the base pairing features\n",
    "\n",
    "\n",
    "# Replace NaNs with 0 probability\n",
    "bpp_set = bpp_set.fillna(0)\n",
    "\n",
    "# Target, classification value\n",
    "y = dataset['NSUN6_affected'].values\n",
    "# Features\n",
    "X = bpp_set.values\n",
    "\n",
    "# Standardiser?\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "principalComponents = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Feature Selection [Statistical Tests (e.g., chi-squared, mutual information)]\n",
    "\n",
    "select the most informative probabilities, might need to do more numerical encoding before statistical test can be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.sort_values of seq         0\n",
      "A           0\n",
      "C           0\n",
      "G           0\n",
      "T           0\n",
      "         ... \n",
      "61-101    219\n",
      "65-96     212\n",
      "21-55     218\n",
      "53-74     216\n",
      "48-83     219\n",
      "Length: 4758, dtype: int64>\n",
      "oh\n"
     ]
    }
   ],
   "source": [
    "    ### Positive Dataset Selection ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Collapsing Probabilities To Reduce Empty Data\n",
    "\n",
    "Aggregated Features: Aggregate probabilities over specific regions (e.g., sliding windows) to reduce dimensionality. For example, compute the mean, variance, or entropy of probabilities within a window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLRNA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
